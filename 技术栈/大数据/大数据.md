# Kudu
    Kudu是一种在 Apache Hadoop 生态系统中使用的存储系统，专门为快速分析处理和实时数据访问而设计。它结合了HDFS（Hadoop Distributed File System）的优点，支持大规模存储，同时也具有类似于传统关系型数据库的快速随机读取和写入能力。
# flink
    Apache Flink 是一种分布式流处理和批处理框架，主要用于大数据实时处理和数据流计算。
    Flink CDC（Change Data Capture）是基于 Apache Flink 和 Debezium 的实时数据采集和处理工具。
# Presto
    Presto是一个分布式SQL查询引擎，主要用于对大规模数据集进行交互式分析。它由Facebook开发，并且现在作为一个开源项目由社区进行维护和发展。
# Apache Paimon
    Apache Paimon 是一个用于流批一体的大规模数据管理系统。它旨在提供高效的数据存储和查询功能，支持实时数据处理和批处理工作负载。

# ZooKeeper
    ZooKeeper 是 Apache 软件基金会的一个软件项目，它为大型分布式计算提供开源的分布式配置服务、同步服务和命名注册。
# Kafka
    Apache Kafka是一个分布式流处理平台，用于构建高性能、可扩展的实时数据流应用程序
# Upsert Kafka SQL Connector
    https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/connectors/table/upsert-kafka/
# DataGen SQL Connector
    https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/connectors/table/datagen/
# Print SQL Connector
    https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/connectors/table/print/
# 
# CAP
    CAP 理论指出对于一个分布式计算系统来说，不可能同时满足以下三点：
    一致性：在分布式环境中，一致性是指数据在多个副本之间是否能够保持一致的特性，等同于所有节点访问同一份最新的数据副本。在一致性的需求下，当一个系统在数据一致的状态下执行更新操作后，应该保证系统的数据仍然处于一致的状态。
    可用性：每次请求都能获取到正确的响应，但是不保证获取的数据为最新数据。
    分区容错性：分布式系统在遇到任何网络分区故障的时候，仍然需要能够保证对外提供满足一致性和可用性的服务，除非是整个网络环境都发生了故障。
    一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项。
    在这三个基本需求中，最多只能同时满足其中的两项，P 是必须的，因此只能在 CP 和 AP 中选择，zookeeper 保证的是 CP，对比 spring cloud 系统中的注册中心 eruka 实现的是 AP。
# BASE 理论
    BASE 是 Basically Available(基本可用)、Soft-state(软状态) 和 Eventually Consistent(最终一致性) 三个短语的缩写。
    基本可用：在分布式系统出现故障，允许损失部分可用性（服务降级、页面降级）。
    软状态：允许分布式系统出现中间状态。而且中间状态不影响系统的可用性。这里的中间状态是指不同的 data replication（数据备份节点）之间的数据更新可以出现延时的最终一致性。
    最终一致性：data replications 经过一段时间达到一致性。
    BASE 理论是对 CAP 中的一致性和可用性进行一个权衡的结果，理论的核心思想就是：我们无法做到强一致，但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性。
# Apache DolphinScheduler
    Apache DolphinScheduler是一个分布式去中心化易扩展的工作流任务调度系统。
# Apache Iceberg
    Apache Iceberg 是一种用于大型分析数据集的开放 Table Format（表格式）。专为大型表设计，单表可以处理 PB 级的数据。Iceberg 旨在解决最终一致的云对象存储中的正确性问题，Iceberg 可以在没有分布式 SQL 的情况下读取 PB 级别的表。
# 流处理和批处理的区别如下：

## 批处理（Batch Processing）：
    批处理是指在某一时间点处理大量数据的手段。
    它通常涉及到对大量静止的（不再变化的）数据集进行一次性的处理。
    批处理作业通常在数据集完整可用后开始执行，并且经常是在数据仓库中进行。
    例如，一个电商平台可能在一天结束时运行一个批处理作业来处理当天所有的交易记录。
## 流处理（Stream Processing）：
    流处理是指对数据实时进行处理，通常是数据生成或接收的同时立即进行。
    流处理适用于连续的数据输入，这些数据一直在变化，需要立即响应。
    例如，社交媒体平台在接收到新的帖子时，可能会实时分析这些帖子的内容和流行趋势。
# Flink运行模式
    可以运行在一台机器上，称为本地（单机）模式；
    也可以使用YARN作为底层资源调度系统以分布式的方式在集群中运行，称为Flink On YARN模式；
    还可以使用Flink自带的资源调度系统，不依赖其他系统，称为Flink Standalone模式。
    还有将Flink部署到Kubernetes的模式，称为Flink On Kubernetes模式。
# dinky
    Dinky 是一个开箱即用的一站式实时计算平台，以 Apache Flink 为基础，连接 OLAP 和数据湖等众多框架,致力于流批一体和湖仓一体的建设与实践。
    Dinky 提供一个轻量级的 IDE 式开发环境，提供一站式开发能力，从语句编写、调试、提交 到 监控、发布、丝滑流畅，解决sql作业文件多，管理困难， 编写困难等问题，还支持智能代码提示，Env参数，全局变量等，让开发更简单，顺滑。
    Dinky无缝支持流批一体，Yarn，K8s，Standalone，任务提交管理全方位支持，运维中心对原有Flink webui进行增强，持久化监控，个性化告警规则配置，智能重启，停止与savepoint管理等。
# Apache Flink和Dinky
    Apache Flink和Dinky是两个不同的项目，但它们可以协同工作以实现实时数据处理和分析。
    集成：Dinky是基于Flink开发的，利用了Flink强大的流处理能力。Dinky的主要目的是简化Flink的使用，因此它提供了很多高层次的工具和接口，让用户可以更方便地创建、管理和监控Flink作业。
    增强：Dinky通过提供图形化界面、SQL支持和其他管理工具，增强了Flink的可用性和易用性，特别是对于那些可能不熟悉Flink底层API的用户。
# Debezium
    Debezium 是一个 CDC（Changelog Data Capture，变更数据捕获）的工具，可以把来自 MySQL、PostgreSQL、Oracle、Microsoft SQL Server 和许多其他数据库的更改实时流式传输到 Kafka 中。 Debezium 为变更日志提供了统一的格式结构，并支持使用 JSON 和 Apache Avro 序列化消息。
    Flink 支持将 Debezium JSON 和 Avro 消息解析为 INSERT / UPDATE / DELETE 消息到 Flink SQL 系统中。
    Flink 还支持将 Flink SQL 中的 INSERT / UPDATE / DELETE 消息编码为 Debezium 格式的 JSON 或 Avro 消息，输出到 Kafka 等存储中。
# Rancher
    Rancher 是供采用容器的团队使用的完整软件堆栈。它解决了管理多个Kubernetes集群的运营和安全挑战，并为DevOps团队提供用于运行容器化工作负载的集成工具。

# flink 和 flink cdc 区别
    Flink 是一个广泛应用于实时和批处理的框架，而 Flink CDC 是基于 Flink 的一个扩展，专门用于捕获和处理数据库中的变化数据。
# Apache Flink 提供了多种运行作业的方式，
    其中包括 FlinkSQL、FlinkJar 和 FlinkSQLEnv。
    FlinkSQL 更适合希望用 SQL 快速定义和执行流处理作业的用户。FlinkSQL：支持SET、DML、DDL语法
    FlinkJar 更适合需要自定义和复杂处理逻辑的开发者，使用 Java 或 Scala 编写程序。
    FlinkSQLEnv 则为运行 Flink SQL 提供了一个配置环境的工具，适合需要在执行 SQL 查询前进行环境配置的用户。
        FlinkSQLEnv：支持SET、DDL语法
        FlinkSQLEnv 场景适用于所有作业的SET、DDL语法统一管理的场景，当前FlinkSQLEnv 在SQL编辑器的语句限制在1000行以内
# dinky 和 flink cdc的关系
    Dinky 是一个实时计算平台，基于 Apache Flink 构建，旨在简化 Flink 作业的开发、部署和管理。
    Flink CDC 是 Apache Flink 的一个扩展，用于捕获和处理数据库中的变更数据。
    Dinky 可以与 Flink CDC 结合使用。用户可以通过 Dinky 提交和管理包含 Flink CDC 功能的 Flink 作业，从而实现对数据库变更数据的实时处理。
    Dinky 提供的友好界面和管理功能，使得用户能够更加方便地配置和监控 Flink CDC 作业。
    通过结合使用 Dinky 和 Flink CDC，用户不仅可以实现对数据库变更数据的实时捕获和处理，还可以利用 Dinky 提供的其他功能（如作业调度、报警机制等）来增强整体解决方案的能力和可靠性。
# ETL（Extract, Transform, Load）
# DataX
    DataX 是阿里云 DataWorks数据集成 的开源版本，在阿里巴巴集团内被广泛使用的离线数据同步工具/平台。
# DataWorks
    阿里云DataWorks数据集成是DataX团队在阿里云上的商业化产品，
# doris
    Apache Doris 是一种现代化的大规模并行处理 (MPP) 分析型数据库产品，用于交互式 SQL 查询和 ETL（抽取、转换、加载）处理。它专为高性能实时分析而设计，特别适用于大数据场景。
# CloudCanal
    CloudCanal 是一款支持多种数据源和目标的数据同步工具，提供高效、可靠、实时的数据同步服务。
# azkaban
    Azkaban 是一个开源的工作流调度系统，主要用于协调和管理大规模的数据处理和ETL（抽取、转换、加载）作业。 
    Apache Azkaban 是一个开源的批处理工作流调度系统，用于管理和调度Hadoop生态系统中的任务和作业。
    Azkaban是由Linkedin开源的一个批量工作流任务调度器
# Databend
    Databend 是一种现代化的云原生数据仓库，旨在提供高性能、弹性和可扩展的分析能力。它专为云环境设计，充分利用云计算的优势，提供高效的数据存储和查询服务。
# Apache SeaTunnel
    https://seatunnel.apache.org/
    https://seatunnel.apache.org/zh-CN/docs/about/
    https://github.com/apache/seatunnel
    SeaTunnel是一个非常易用、超高性能的分布式数据集成平台，支持实时海量数据同步。 每天可稳定高效同步数百亿数据，已被近百家企业应用于生产。
# Apache Paimon
    Apache Paimon (incubating) 是一项流式数据湖存储技术，可以为用户提供高吞吐、低延迟的数据摄入、流式订阅以及实时查询能力
    Paimon 是一个用于管理大规模数据存储的表格式，因此可以方便地管理和查询实时数据流。
# seatunnel 实时获取TIDB业务库数据并写入 Paimon
# starrocks 访问 Paimon 数据，并能与starrocks数据做关联查询
# 将 starrocks 结果表数据写入 TIDB

# DolphinScheduler
    Apache DolphinScheduler 是一个分布式和可扩展的开源工作流协调平台，具有强大的DAG可视化界面

# seatunnel-web
    https://github.com/apache/seatunnel-web
